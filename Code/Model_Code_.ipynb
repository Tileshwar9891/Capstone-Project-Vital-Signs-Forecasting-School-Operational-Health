{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201390a-5d60-444b-b480-816b78cb08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828303f8-1979-4282-8ed5-0a7ffa1a41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "file_path = r\"C:\\Users\\tiles\\Downloads\\final_dataset 8.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2deed2-863e-4d42-aaf3-ec3455deb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating target variables\n",
    "df.loc[df['TS-Capacity']==0, 'TS-Capacity'] = 216  \n",
    "df['perc_capacity'] = df['TS-Enrollment']/df['TS-Capacity']\n",
    "df['net_rev_per_student'] = df['net_program_80']/df['TS-Enrollment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6529bf-92f6-489c-9a61-801e0f2c5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling NA\n",
    "fill_values = {\n",
    "    'fulcrum_total_app': 0, \n",
    "    'fulcrum_grant_count': 0, \n",
    "    'fulcrum_grant_mean': 0,\n",
    "    'Nonfamily Avg Household Size': df.groupby('Year')['Nonfamily Avg Household Size'].transform('mean'), \n",
    "    'Med inc_Med income families': df['Med inc_all households']\n",
    "}\n",
    "# Checking filling missing values \n",
    "df = df.fillna(value=fill_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700a2cd-a7e8-4a39-9241-63d37212dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop which can affect the overall model development \n",
    "cols_to_drop = [\n",
    "    'Unnamed: 0', 'school_id', 'Name', 'zip_code', 'Type', 'type', 'level', 'District', 'School',\n",
    "    'TS-Catholic', 'TS-Non-Catholic', 'Income At or Above Poverty Level', \n",
    "    'TPS-Non-Catholic', 'TPS-Catholic', 'Female (Female)', 'Male (Male)',\n",
    "    'child_under_5', 'child_5_9', 'child_10_14', \n",
    "    'pub_enroll_all_students',\n",
    "    'PercentLevel1_ELA', 'PercentLevel1_Math', 'PercentLevel2_ELA', 'PercentLevel2_Math',\n",
    "    'PercentLevel3_ELA', 'PercentLevel3_Math', 'PercentLevel4_ELA', 'PercentLevel4_Math',\n",
    "    'parish_ID', 'parish_name&city', 'short_name', 'Year_x', 'Parish', 'TS-Capacity', 'TS-Enrollment'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda5582-af5a-45d5-992c-0999fe2689a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the current data\n",
    "df_filtered = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Filtered shape: {df_filtered.shape}\")\n",
    "print(f\"Removed {len([col for col in cols_to_drop if col in df.columns])} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a446f-0272-4c48-9901-e1a5fcea8ec7",
   "metadata": {},
   "source": [
    "#### Variable Importance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba72b6-0ee4-4589-b643-b3f362dfe8e1",
   "metadata": {},
   "source": [
    "##### T - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463373d-c21b-4010-a467-1d491e48a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-test\n",
    "def perform_t_test(df, features, target, threshold=0.05):\n",
    "    t_test_results = {}\n",
    "    \n",
    "    # Median of target variable to split data\n",
    "    median_target = df[target].median()\n",
    "    group_high = df[df[target] > median_target]\n",
    "    group_low = df[df[target] <= median_target]\n",
    "    \n",
    "    significant_features = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if df[feature].isna().sum() > 0:\n",
    "            continue\n",
    "        if df[feature].dtype not in [np.float64, np.int64]:\n",
    "            continue\n",
    "            \n",
    "        values_high = group_high[feature].values\n",
    "        values_low = group_low[feature].values\n",
    "        \n",
    "        # Performing t-test\n",
    "        t_stat, p_value = stats.ttest_ind(values_high, values_low, equal_var=False)\n",
    "        \n",
    "        t_test_results[feature] = {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < threshold\n",
    "        }\n",
    "        \n",
    "        if p_value < threshold:\n",
    "            significant_features.append(feature)\n",
    "    \n",
    "    return t_test_results, significant_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf9c8e-7120-478b-8f5a-d99679b2103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting numeric features \n",
    "numeric_features = df_filtered.select_dtypes(include=[np.float64, np.int64]).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in ['perc_capacity', 'net_rev_per_student']]\n",
    "\n",
    "print(f\"\\nNumber of filtered numeric features: {len(numeric_features)}\")\n",
    "\n",
    "# Performing t-test for both target variables\n",
    "print(\"\\nPerforming T-test for perc_capacity...\")\n",
    "t_test_perc_capacity, sig_features_perc_capacity = perform_t_test(df_filtered, numeric_features, 'perc_capacity')\n",
    "\n",
    "print(\"\\nPerforming T-test for net_rev_per_student...\")\n",
    "t_test_net_rev, sig_features_net_rev = perform_t_test(df_filtered, numeric_features, 'net_rev_per_student')\n",
    "\n",
    "# Significance of features\n",
    "print(f\"\\nSignificant features for perc_capacity: {len(sig_features_perc_capacity)}\")\n",
    "sig_perc_capacity_df = pd.DataFrame({\n",
    "    'Feature': list(t_test_perc_capacity.keys()), \n",
    "    'P-value': [t_test_perc_capacity[f]['p_value'] for f in t_test_perc_capacity],\n",
    "    'Significant': [t_test_perc_capacity[f]['significant'] for f in t_test_perc_capacity]\n",
    "})\n",
    "sig_perc_capacity_df = sig_perc_capacity_df.sort_values('P-value')\n",
    "print(sig_perc_capacity_df[sig_perc_capacity_df['Significant'] == True].head(15))\n",
    "\n",
    "print(f\"\\nSignificant features for net_rev_per_student: {len(sig_features_net_rev)}\")\n",
    "sig_net_rev_df = pd.DataFrame({\n",
    "    'Feature': list(t_test_net_rev.keys()), \n",
    "    'P-value': [t_test_net_rev[f]['p_value'] for f in t_test_net_rev],\n",
    "    'Significant': [t_test_net_rev[f]['significant'] for f in t_test_net_rev]\n",
    "})\n",
    "sig_net_rev_df = sig_net_rev_df.sort_values('P-value')\n",
    "print(sig_net_rev_df[sig_net_rev_df['Significant'] == True].head(15))\n",
    "\n",
    "# Common Significant features\n",
    "common_sig_features = list(set(sig_features_perc_capacity) & set(sig_features_net_rev))\n",
    "print(f\"\\nCommon significant features between both targets: {len(common_sig_features)}\")\n",
    "print(common_sig_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4d297-1e91-46e6-a861-f09e3d574c5d",
   "metadata": {},
   "source": [
    "#### Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b86ac-8ba7-4e09-a8be-452df298fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Spearman correlation with target variable\n",
    "def perform_spearman_correlation(df, features, target):\n",
    "    corr_results = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        if df[feature].isna().sum() == 0:  # Skip features with missing values\n",
    "            correlation, p_value = stats.spearmanr(df[feature], df[target])\n",
    "            corr_results[feature] = {\n",
    "                'correlation': correlation,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    corr_df = pd.DataFrame({\n",
    "        'Feature': list(corr_results.keys()),\n",
    "        'Correlation': [corr_results[f]['correlation'] for f in corr_results],\n",
    "        'P-value': [corr_results[f]['p_value'] for f in corr_results],\n",
    "        'Significant': [corr_results[f]['significant'] for f in corr_results]\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    corr_df['Abs_Correlation'] = corr_df['Correlation'].abs()\n",
    "    corr_df = corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "    \n",
    "    return corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c19b9-1475-4e63-a9cb-8e048a6877f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric features \n",
    "numeric_features = df_filtered.select_dtypes(include=[np.float64, np.int64]).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in ['perc_capacity', 'net_rev_per_student']]\n",
    "\n",
    "print(f\"Number of filtered numeric features for Spearman correlation: {len(numeric_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6314a0-ba0a-4f9c-af29-ccd6a04f6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman Correlation Calculation\n",
    "# Calculate Spearman correlation for perc_capacity\n",
    "print(\"\\nCalculating Spearman correlation for perc_capacity...\")\n",
    "corr_perc_capacity = perform_spearman_correlation(df_filtered, numeric_features, 'perc_capacity')\n",
    "\n",
    "# Calculate Spearman correlation for net_rev_per_student\n",
    "print(\"\\nCalculating Spearman correlation for net_rev_per_student...\")\n",
    "corr_net_rev = perform_spearman_correlation(df_filtered, numeric_features, 'net_rev_per_student')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad69d85-0681-4e63-afb5-b211c49b07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significant features with strongest correlations\n",
    "print(\"\\nTop significant features by Spearman correlation for perc_capacity:\")\n",
    "sig_perc_corr = corr_perc_capacity[corr_perc_capacity['Significant'] == True]\n",
    "print(sig_perc_corr[['Feature', 'Correlation', 'P-value']].head(15))\n",
    "\n",
    "print(\"\\nTop significant features by Spearman correlation for net_rev_per_student:\")\n",
    "sig_net_rev_corr = corr_net_rev[corr_net_rev['Significant'] == True]\n",
    "print(sig_net_rev_corr[['Feature', 'Correlation', 'P-value']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890355c-bd77-4709-a706-73df1e012de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with significant correlation for both targets\n",
    "sig_perc_features = set(sig_perc_corr['Feature'].tolist())\n",
    "sig_net_rev_features = set(sig_net_rev_corr['Feature'].tolist())\n",
    "common_sig_corr_features = list(sig_perc_features.intersection(sig_net_rev_features))\n",
    "\n",
    "print(f\"\\nFeatures with significant Spearman correlation for both targets: {len(common_sig_corr_features)}\")\n",
    "print(common_sig_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eb8a4-e9fe-49f7-942d-6e42a9f2e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Permutation importance analysis\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987ebc3-67ea-45b8-8414-27bb9be9c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric features from filtered dataset\n",
    "numeric_features = df_filtered.select_dtypes(include=[np.float64, np.int64]).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in ['perc_capacity', 'net_rev_per_student']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad0251-83b6-4748-a81d-7b74c8dff978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for permutation importance\n",
    "X = df_filtered[numeric_features].copy()\n",
    "X = X.fillna(X.mean())  # Fill missing values with mean\n",
    "\n",
    "# Get target variables\n",
    "y_perc_capacity = df_filtered['perc_capacity']\n",
    "y_net_rev = df_filtered['net_rev_per_student']\n",
    "\n",
    "print(\"Prepared data for Random Forest permutation importance analysis\")\n",
    "print(f\"Number of features being evaluated: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb01ec7-b4da-455f-bfbf-6e7a906d0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate permutation importance\n",
    "def get_permutation_importance(X, y, feature_names):\n",
    "    # Split the data - we need a test set for permutation importance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate R2 and RMSE on test set (informational only)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Calculate permutation importance on test set\n",
    "    # This shows how much model performance decreases when a feature is randomly shuffled\n",
    "    perm_importance = permutation_importance(rf, X_test, y_test, \n",
    "                                            n_repeats=5,  # Fewer repeats to speed up calculation\n",
    "                                            random_state=42,\n",
    "                                            n_jobs=-1)\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm_importance.importances_mean,\n",
    "        'Std': perm_importance.importances_std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return importance_df, r2, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeaaf9f-6436-4554-b076-4b634c257860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance for perc_capacity\n",
    "print(\"\\nCalculating permutation importance for perc_capacity...\")\n",
    "perm_imp_perc, r2_perc, rmse_perc = get_permutation_importance(X, y_perc_capacity, numeric_features)\n",
    "\n",
    "# Calculate permutation importance for net_rev_per_student\n",
    "print(\"Calculating permutation importance for net_rev_per_student...\")\n",
    "perm_imp_net, r2_net, rmse_net = get_permutation_importance(X, y_net_rev, numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c2ef4-2a08-4307-92a8-ff0dca334e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for perc_capacity\n",
    "print(\"\\nPermutation importance for perc_capacity:\")\n",
    "print(f\"Model R² Score: {r2_perc:.4f}, RMSE: {rmse_perc:.4f}\")\n",
    "print(\"\\nTop 15 features by permutation importance:\")\n",
    "print(perm_imp_perc.head(15))\n",
    "\n",
    "# Display results for net_rev_per_student\n",
    "print(\"\\nPermutation importance for net_rev_per_student:\")\n",
    "print(f\"Model R² Score: {r2_net:.4f}, RMSE: {rmse_net:.4f}\")\n",
    "print(\"\\nTop 15 features by permutation importance:\")\n",
    "print(perm_imp_net.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23043a6c-15f4-448e-9fa0-9656376bdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to variables with clear names for later meta-analysis\n",
    "rf_perm_imp_features_perc_capacity = perm_imp_perc.head(15)['Feature'].tolist()\n",
    "rf_perm_imp_features_net_rev = perm_imp_net.head(15)['Feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ade15-b362-475a-9330-c4de29b0e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability Selection for Feature Importance\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629eaee-5a08-41ff-9ac2-2f2cf991792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric features from filtered dataset\n",
    "numeric_features = df_filtered.select_dtypes(include=[np.float64, np.int64]).columns.tolist()\n",
    "numeric_features = [col for col in numeric_features if col not in ['perc_capacity', 'net_rev_per_student']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95501596-0654-4403-abee-d2ebcce1be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_filtered[numeric_features].copy()\n",
    "X = X.fillna(X.mean())  # Fill missing values with mean\n",
    "\n",
    "# Get target variables\n",
    "y_perc_capacity = df_filtered['perc_capacity']\n",
    "y_net_rev = df_filtered['net_rev_per_student']\n",
    "\n",
    "print(\"Prepared data for Stability Selection\")\n",
    "print(f\"Number of features being evaluated: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41d5f6-16c9-4a20-959f-047b3ad12638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform stability selection\n",
    "def stability_selection(X, y, feature_names, n_subsamples=100, sample_fraction=0.75, threshold=0.5):\n",
    "    n_samples, n_features = X.shape\n",
    "    subsample_size = int(n_samples * sample_fraction)\n",
    "    \n",
    "    # Track feature selection frequency\n",
    "    selection_frequency = np.zeros(n_features)\n",
    "    \n",
    "    for i in range(n_subsamples):\n",
    "        # Random subsample\n",
    "        subsample_indices = np.random.choice(n_samples, subsample_size, replace=False)\n",
    "        X_subsample = X.iloc[subsample_indices]\n",
    "        y_subsample = y.iloc[subsample_indices]\n",
    "        \n",
    "        # Find optimal alpha with cross-validation\n",
    "        lasso_cv = LassoCV(cv=5, random_state=i)\n",
    "        lasso_cv.fit(X_subsample, y_subsample)\n",
    "        \n",
    "        # Fit Lasso with selected alpha\n",
    "        lasso = Lasso(alpha=lasso_cv.alpha_)\n",
    "        lasso.fit(X_subsample, y_subsample)\n",
    "        \n",
    "        # Track which features were selected (non-zero coefficients)\n",
    "        selection_frequency += (lasso.coef_ != 0).astype(int)\n",
    "    \n",
    "    # Calculate selection probability for each feature\n",
    "    selection_probability = selection_frequency / n_subsamples\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Selection_Probability': selection_probability\n",
    "    })\n",
    "    \n",
    "    # Sort by selection probability\n",
    "    results = results.sort_values('Selection_Probability', ascending=False)\n",
    "    \n",
    "    # Get stable features based on threshold\n",
    "    stable_features = results[results['Selection_Probability'] >= threshold]['Feature'].tolist()\n",
    "    \n",
    "    return results, stable_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36fc533-4c36-452f-a0ff-633831773750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stability selection for perc_capacity\n",
    "print(\"\\nPerforming stability selection for perc_capacity...\")\n",
    "stability_results_perc, stable_features_perc = stability_selection(\n",
    "    X, y_perc_capacity, numeric_features, n_subsamples=50  # Reduced for speed\n",
    ")\n",
    "\n",
    "# Perform stability selection for net_rev_per_student\n",
    "print(\"Performing stability selection for net_rev_per_student...\")\n",
    "stability_results_net, stable_features_net = stability_selection(\n",
    "    X, y_net_rev, numeric_features, n_subsamples=50  # Reduced for speed\n",
    ")\n",
    "\n",
    "# Show results\n",
    "print(\"\\nStability selection results for perc_capacity:\")\n",
    "print(f\"Number of stable features: {len(stable_features_perc)}\")\n",
    "print(stability_results_perc.head(15))\n",
    "\n",
    "print(\"\\nStability selection results for net_rev_per_student:\")\n",
    "print(f\"Number of stable features: {len(stable_features_net)}\")\n",
    "print(stability_results_net.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d8bee-8df9-40b7-8210-b4e785294ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Analysis of Feature Selection Methods\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e077b8f-fb45-4a56-97e4-7a2db24b3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. From T-test\n",
    "t_test_features_perc = sig_perc_capacity_df[sig_perc_capacity_df['Significant'] == True]['Feature'].tolist()\n",
    "print(f\"\\nT-test significant features for perc_capacity: {len(t_test_features_perc)}\")\n",
    "\n",
    "# 2. From Spearman Correlation\n",
    "spearman_features_perc = sig_perc_corr['Feature'].tolist()\n",
    "print(f\"Spearman significant features for perc_capacity: {len(spearman_features_perc)}\")\n",
    "\n",
    "# 3. From Random Forest Permutation Importance (top 15)\n",
    "rf_features_perc = perm_imp_perc.head(15)['Feature'].tolist()\n",
    "print(f\"Random Forest important features for perc_capacity: {len(rf_features_perc)}\")\n",
    "\n",
    "# 4. From Stability Selection\n",
    "stability_features_perc = stability_results_perc[stability_results_perc['Selection_Probability'] >= 0.5]['Feature'].tolist()\n",
    "print(f\"Stability Selection stable features for perc_capacity: {len(stability_features_perc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91940141-8109-4841-a65c-fff39d425ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For net_rev_per_student target:\n",
    "# 1. From T-test\n",
    "t_test_features_net = sig_net_rev_df[sig_net_rev_df['Significant'] == True]['Feature'].tolist()\n",
    "print(f\"\\nT-test significant features for net_rev_per_student: {len(t_test_features_net)}\")\n",
    "\n",
    "# 2. From Spearman Correlation\n",
    "spearman_features_net = sig_net_rev_corr['Feature'].tolist()\n",
    "print(f\"Spearman significant features for net_rev_per_student: {len(spearman_features_net)}\")\n",
    "\n",
    "# 3. From Random Forest Permutation Importance (top 15)\n",
    "rf_features_net = perm_imp_net.head(15)['Feature'].tolist()\n",
    "print(f\"Random Forest important features for net_rev_per_student: {len(rf_features_net)}\")\n",
    "\n",
    "# 4. From Stability Selection\n",
    "stability_features_net = stability_results_net[stability_results_net['Selection_Probability'] >= 0.5]['Feature'].tolist()\n",
    "print(f\"Stability Selection stable features for net_rev_per_student: {len(stability_features_net)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e6477-bf3f-425b-8201-3b662a940085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature scoring system for each target\n",
    "# For each feature, count how many methods selected it\n",
    "feature_counts_perc = {}\n",
    "feature_counts_net = {}\n",
    "\n",
    "# Get all possible features\n",
    "all_features = set(t_test_features_perc + spearman_features_perc + rf_features_perc + stability_features_perc + \n",
    "                  t_test_features_net + spearman_features_net + rf_features_net + stability_features_net)\n",
    "\n",
    "# Count occurrences for each feature for perc_capacity\n",
    "for feature in all_features:\n",
    "    # Count for perc_capacity\n",
    "    count_perc = 0\n",
    "    if feature in t_test_features_perc:\n",
    "        count_perc += 1\n",
    "    if feature in spearman_features_perc:\n",
    "        count_perc += 1\n",
    "    if feature in rf_features_perc:\n",
    "        count_perc += 1\n",
    "    if feature in stability_features_perc:\n",
    "        count_perc += 1\n",
    "    \n",
    "    feature_counts_perc[feature] = count_perc\n",
    "    \n",
    "    # Count for net_rev_per_student\n",
    "    count_net = 0\n",
    "    if feature in t_test_features_net:\n",
    "        count_net += 1\n",
    "    if feature in spearman_features_net:\n",
    "        count_net += 1\n",
    "    if feature in rf_features_net:\n",
    "        count_net += 1\n",
    "    if feature in stability_features_net:\n",
    "        count_net += 1\n",
    "    \n",
    "    feature_counts_net[feature] = count_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7822758-43fc-46c3-83b4-c85780395672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "feature_scores_perc = pd.DataFrame({\n",
    "    'Feature': list(feature_counts_perc.keys()),\n",
    "    'Selection_Count': list(feature_counts_perc.values())\n",
    "}).sort_values('Selection_Count', ascending=False)\n",
    "\n",
    "feature_scores_net = pd.DataFrame({\n",
    "    'Feature': list(feature_counts_net.keys()),\n",
    "    'Selection_Count': list(feature_counts_net.values())\n",
    "}).sort_values('Selection_Count', ascending=False)\n",
    "\n",
    "# Display features selected by multiple methods\n",
    "print(\"\\nTop features for perc_capacity (selected by multiple methods):\")\n",
    "print(feature_scores_perc[feature_scores_perc['Selection_Count'] >= 2].head(15))\n",
    "\n",
    "print(\"\\nTop features for net_rev_per_student (selected by multiple methods):\")\n",
    "print(feature_scores_net[feature_scores_net['Selection_Count'] >= 2].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e98cd1-6752-4023-8354-eb17d730bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features that appeared in at least 2 methods\n",
    "selected_features_perc = feature_scores_perc[feature_scores_perc['Selection_Count'] >= 2]['Feature'].tolist()\n",
    "selected_features_net = feature_scores_net[feature_scores_net['Selection_Count'] >= 2]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\nSelected features for perc_capacity: {len(selected_features_perc)}\")\n",
    "print(selected_features_perc)\n",
    "\n",
    "print(f\"\\nSelected features for net_rev_per_student: {len(selected_features_net)}\")\n",
    "print(selected_features_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd35c10-639d-4da3-9d54-3330cc589f22",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef79921-0c48-4337-8223-8fb2cd0cb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis Classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c03ac-4c0f-4610-b272-62e023f2140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define selected features for each target variable\n",
    "selected_features_perc = ['tuition_fees_finaid_scholarships', 'benefits_52xx', 'Med inc_all households', 'net_program_80', 'salaries_51xx', 'total_expenses_80', 'program_expenses', 'supplies', 'total_revenue_80', 'all_other_expenses', 'T-Parish', 'repairs_maintenance_58xx', 'female_young_adults', 'all_other_revenue', 'contracted_services', 'fundraising_parents_club', 'utilities', 'TPS-Employment', 'bequests', 'PercentMetStandard_ELA', 'Med inc_Med income families', 'business_revenue_45xx', 'PercentMetStandard_Math', 'Nonfamily Total Households', 'Nonfamily Avg Household Size', 'male_young_adults']\n",
    "\n",
    "selected_features_net = ['net_program_80', 'all_other_revenue', 'Med inc_Med income families', 'TPS-Employment', 'T-Parish', 'tuition_fees_finaid_scholarships', 'Owner-Occupied Units', 'total_expenses_80', 'Married Avg Family Size', 'TS_Non_catholic_ratio', 'Married Avg Household Size', 'Avg Household Size', 'total_revenue_80', 'Avg Family Size', 'gift_revenue_44xx', 'fundraising_parents_club']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99255734-bfd4-4be7-95ed-9235ae3103fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "# Convert continuous targets to categorical (binary for simplicity)\n",
    "# Calculate median of target variables to split into binary classes\n",
    "perc_capacity_median = df_filtered['perc_capacity'].median()\n",
    "net_rev_median = df_filtered['net_rev_per_student'].median()\n",
    "\n",
    "# Create binary target variables\n",
    "df_filtered['perc_capacity_binary'] = (df_filtered['perc_capacity'] > perc_capacity_median).astype(int)\n",
    "df_filtered['net_rev_binary'] = (df_filtered['net_rev_per_student'] > net_rev_median).astype(int)\n",
    "\n",
    "print(f\"\\nClass distribution for perc_capacity_binary:\")\n",
    "print(df_filtered['perc_capacity_binary'].value_counts())\n",
    "\n",
    "print(f\"\\nClass distribution for net_rev_binary:\")\n",
    "print(df_filtered['net_rev_binary'].value_counts())\n",
    "\n",
    "# Function to build and evaluate LDA model\n",
    "def build_lda_model(X, y, feature_names):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Create a pipeline with scaling and LDA\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lda', LinearDiscriminantAnalysis())\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Get LDA coefficients for feature importance\n",
    "    lda = pipeline.named_steps['lda']\n",
    "    scaler = pipeline.named_steps['scaler']\n",
    "    \n",
    "    # Get coefficient importance (use absolute values)\n",
    "    coef = np.abs(lda.coef_[0])\n",
    "    \n",
    "    # Scale coefficients by the scaler to get feature importance\n",
    "    scaled_coef = coef * scaler.scale_\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': scaled_coef\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'class_report': class_report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'cv_scores': cv_scores,\n",
    "        'feature_importance': feature_importance\n",
    "    }\n",
    "\n",
    "# Prepare data for perc_capacity model using selected features\n",
    "print(\"\\n==== Building LDA model for perc_capacity ====\")\n",
    "# Filter features that exist in the dataset\n",
    "valid_perc_features = [f for f in selected_features_perc if f in df_filtered.columns]\n",
    "print(f\"Valid features for perc_capacity: {len(valid_perc_features)}/{len(selected_features_perc)}\")\n",
    "\n",
    "X_perc = df_filtered[valid_perc_features].copy()\n",
    "X_perc = X_perc.fillna(X_perc.mean())  # Fill missing values\n",
    "y_perc = df_filtered['perc_capacity_binary']\n",
    "\n",
    "# Build LDA model for perc_capacity\n",
    "lda_perc = build_lda_model(X_perc, y_perc, valid_perc_features)\n",
    "\n",
    "# Prepare data for net_rev model using selected features\n",
    "print(\"\\n==== Building LDA model for net_rev_per_student ====\")\n",
    "# Filter features that exist in the dataset\n",
    "valid_net_features = [f for f in selected_features_net if f in df_filtered.columns]\n",
    "print(f\"Valid features for net_rev_per_student: {len(valid_net_features)}/{len(selected_features_net)}\")\n",
    "\n",
    "X_net = df_filtered[valid_net_features].copy()\n",
    "X_net = X_net.fillna(X_net.mean())  # Fill missing values\n",
    "y_net = df_filtered['net_rev_binary']\n",
    "\n",
    "# Build LDA model for net_rev_per_student\n",
    "lda_net = build_lda_model(X_net, y_net, valid_net_features)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n==== LDA Results for perc_capacity ====\")\n",
    "print(f\"Test Accuracy: {lda_perc['accuracy']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {lda_perc['cv_scores'].mean():.4f} ± {lda_perc['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(lda_perc['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(lda_perc['feature_importance'].head(10))\n",
    "\n",
    "print(\"\\n==== LDA Results for net_rev_per_student ====\")\n",
    "print(f\"Test Accuracy: {lda_net['accuracy']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {lda_net['cv_scores'].mean():.4f} ± {lda_net['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(lda_net['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(lda_net['feature_importance'].head(10))\n",
    "\n",
    "# Visualize LDA results\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot confusion matrices\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(lda_perc['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - perc_capacity')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(lda_net['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - net_rev_per_student')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Plot feature importance\n",
    "plt.subplot(2, 2, 3)\n",
    "top10_perc = lda_perc['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_perc)\n",
    "plt.title('Top 10 Features - perc_capacity LDA')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "top10_net = lda_net['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_net)\n",
    "plt.title('Top 10 Features - net_rev_per_student LDA')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lda_results_separate.png')\n",
    "plt.show()\n",
    "\n",
    "# Compare model performance across targets\n",
    "models = ['perc_capacity', 'net_rev_per_student']\n",
    "accuracies = [\n",
    "    lda_perc['accuracy'], \n",
    "    lda_net['accuracy']\n",
    "]\n",
    "cv_accuracies = [\n",
    "    lda_perc['cv_scores'].mean(),\n",
    "    lda_net['cv_scores'].mean()\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, accuracies, width, label='Test Accuracy')\n",
    "plt.bar(x + width/2, cv_accuracies, width, label='CV Accuracy')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LDA Model Performance Comparison')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('lda_performance_comparison_separate.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLDA classification analysis completed with separate feature sets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9677595-2a77-49b1-bec7-0537632ef7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest and SVM Classification Models - Separate Features Only\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Note: We're using the feature lists defined in the previous code\n",
    "# No need to redefine selected_features_perc and selected_features_net\n",
    "\n",
    "# Function to build and evaluate classification models\n",
    "def build_classification_model(X, y, feature_names, model_type='rf'):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Create a pipeline with scaling and the classifier\n",
    "    if model_type == 'rf':\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "        ])\n",
    "    elif model_type == 'svm':\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', SVC(probability=True, random_state=42))\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'rf' or 'svm'\")\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Get feature importance (for Random Forest only)\n",
    "    if model_type == 'rf':\n",
    "        rf = pipeline.named_steps['classifier']\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': rf.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    else:\n",
    "        feature_importance = None\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'class_report': class_report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'cv_scores': cv_scores,\n",
    "        'feature_importance': feature_importance,\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "\n",
    "# Prepare data for models - Filter features that exist in the dataset\n",
    "valid_perc_features = [f for f in selected_features_perc if f in df_filtered.columns]\n",
    "valid_net_features = [f for f in selected_features_net if f in df_filtered.columns]\n",
    "\n",
    "print(f\"\\nValid features for perc_capacity: {len(valid_perc_features)}/{len(selected_features_perc)}\")\n",
    "print(f\"Valid features for net_rev_per_student: {len(valid_net_features)}/{len(selected_features_net)}\")\n",
    "\n",
    "# Prepare data for perc_capacity model\n",
    "X_perc = df_filtered[valid_perc_features].copy()\n",
    "X_perc = X_perc.fillna(X_perc.mean())  # Fill missing values\n",
    "y_perc = df_filtered['perc_capacity_binary']\n",
    "\n",
    "# Prepare data for net_rev model\n",
    "X_net = df_filtered[valid_net_features].copy()\n",
    "X_net = X_net.fillna(X_net.mean())  # Fill missing values\n",
    "y_net = df_filtered['net_rev_binary']\n",
    "\n",
    "# Build Random Forest models\n",
    "print(\"\\n==== Building Random Forest models ====\")\n",
    "rf_perc = build_classification_model(X_perc, y_perc, valid_perc_features, model_type='rf')\n",
    "rf_net = build_classification_model(X_net, y_net, valid_net_features, model_type='rf')\n",
    "\n",
    "# Build SVM models\n",
    "print(\"\\n==== Building SVM models ====\")\n",
    "svm_perc = build_classification_model(X_perc, y_perc, valid_perc_features, model_type='svm')\n",
    "svm_net = build_classification_model(X_net, y_net, valid_net_features, model_type='svm')\n",
    "\n",
    "# Display Random Forest results\n",
    "print(\"\\n==== Random Forest Results for perc_capacity ====\")\n",
    "print(f\"Test Accuracy: {rf_perc['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {rf_perc['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {rf_perc['cv_scores'].mean():.4f} ± {rf_perc['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(rf_perc['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(rf_perc['feature_importance'].head(10))\n",
    "\n",
    "print(\"\\n==== Random Forest Results for net_rev_per_student ====\")\n",
    "print(f\"Test Accuracy: {rf_net['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {rf_net['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {rf_net['cv_scores'].mean():.4f} ± {rf_net['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(rf_net['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(rf_net['feature_importance'].head(10))\n",
    "\n",
    "# Display SVM results\n",
    "print(\"\\n==== SVM Results for perc_capacity ====\")\n",
    "print(f\"Test Accuracy: {svm_perc['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {svm_perc['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {svm_perc['cv_scores'].mean():.4f} ± {svm_perc['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(svm_perc['class_report'])\n",
    "\n",
    "print(\"\\n==== SVM Results for net_rev_per_student ====\")\n",
    "print(f\"Test Accuracy: {svm_net['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {svm_net['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {svm_net['cv_scores'].mean():.4f} ± {svm_net['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(svm_net['class_report'])\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Plot confusion matrices for Random Forest\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(rf_perc['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('RF Confusion Matrix - perc_capacity')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(rf_net['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('RF Confusion Matrix - net_rev_per_student')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(rf_perc['fpr'], rf_perc['tpr'], label=f'RF (AUC = {rf_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot(svm_perc['fpr'], svm_perc['tpr'], label=f'SVM (AUC = {svm_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - perc_capacity')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(rf_net['fpr'], rf_net['tpr'], label=f'RF (AUC = {rf_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot(svm_net['fpr'], svm_net['tpr'], label=f'SVM (AUC = {svm_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - net_rev_per_student')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_svm_results_separate.png')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance visualization for Random Forest\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# For perc_capacity\n",
    "plt.subplot(1, 2, 1)\n",
    "top10_perc = rf_perc['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_perc)\n",
    "plt.title('RF Top 10 Features - perc_capacity')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "# For net_rev_per_student\n",
    "plt.subplot(1, 2, 2)\n",
    "top10_net = rf_net['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_net)\n",
    "plt.title('RF Top 10 Features - net_rev_per_student')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_feature_importance_separate.png')\n",
    "plt.show()\n",
    "\n",
    "# Compare model performance across methods and targets\n",
    "models = ['RF-perc', 'RF-net', 'SVM-perc', 'SVM-net']\n",
    "accuracies = [rf_perc['accuracy'], rf_net['accuracy'], svm_perc['accuracy'], svm_net['accuracy']]\n",
    "cv_accuracies = [rf_perc['cv_scores'].mean(), rf_net['cv_scores'].mean(), svm_perc['cv_scores'].mean(), svm_net['cv_scores'].mean()]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, accuracies, width, label='Test Accuracy')\n",
    "plt.bar(x + width/2, cv_accuracies, width, label='CV Accuracy')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_separate.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRandom Forest and SVM classification analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6b8d8-ea53-4d9f-b7e4-4a5646fa2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis Classification Script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prepare binary target variables\n",
    "# Calculate median of target variables to split into binary classes\n",
    "perc_capacity_median = df_filtered['perc_capacity'].median()\n",
    "net_rev_median = df_filtered['net_rev_per_student'].median()\n",
    "\n",
    "# Create binary target variables\n",
    "df_filtered['perc_capacity_binary'] = (df_filtered['perc_capacity'] > perc_capacity_median).astype(int)\n",
    "df_filtered['net_rev_binary'] = (df_filtered['net_rev_per_student'] > net_rev_median).astype(int)\n",
    "# Create deficit binary target variable\n",
    "# Create a 3-category financial status variable\n",
    "def create_financial_status(net_program):\n",
    "    if net_program < -10000:\n",
    "        return 1  # Deficit\n",
    "    elif net_program > 10000:\n",
    "        return 2  # Surplus\n",
    "    else:\n",
    "        return 0  # Balanced\n",
    "\n",
    "# Apply the function to create the financial status variable\n",
    "df_filtered['financial_status'] = df_filtered['net_program_80'].apply(create_financial_status)\n",
    "\n",
    "# For backward compatibility, also update the deficit_binary to use the -10000 threshold\n",
    "df_filtered['deficit_binary'] = (df_filtered['net_program_80'] < -10000).astype(int)\n",
    "\n",
    "# Print the distributions of both variables\n",
    "print(\"\\nUpdated deficit_binary distribution:\")\n",
    "print(df_filtered['deficit_binary'].value_counts())\n",
    "print(f\"Percentage of schools with significant deficit (< -$10,000): {df_filtered['deficit_binary'].mean() * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nFinancial status distribution:\")\n",
    "status_counts = df_filtered['financial_status'].value_counts()\n",
    "status_percent = df_filtered['financial_status'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Counts:\")\n",
    "for status, count in status_counts.items():\n",
    "    if status == 0:\n",
    "        status_name = \"Balanced (-$10K to $10K)\"\n",
    "    elif status == 1:\n",
    "        status_name = \"Deficit (< -$10K)\"\n",
    "    else:\n",
    "        status_name = \"Surplus (> $10K)\"\n",
    "    print(f\"{status_name}: {count} ({status_percent[status]:.2f}%)\")\n",
    "print(\"Class distribution for binary target variables:\")\n",
    "print(\"\\nperc_capacity_binary:\")\n",
    "print(df_filtered['perc_capacity_binary'].value_counts())\n",
    "\n",
    "print(\"\\nnet_rev_binary:\")\n",
    "print(df_filtered['net_rev_binary'].value_counts())\n",
    "\n",
    "print(\"\\ndeficit_binary:\")\n",
    "print(df_filtered['deficit_binary'].value_counts())\n",
    "print(f\"Percentage of schools with deficit: {df_filtered['deficit_binary'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Define features for deficit prediction - exclude net_program_80 which is used to calculate the target\n",
    "deficit_features = [f for f in selected_features_net if f != 'net_program_80']\n",
    "\n",
    "# Function to build and evaluate LDA model with ROC AUC calculation\n",
    "def build_lda_model(X, y, feature_names):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Create a pipeline with scaling and LDA\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('lda', LinearDiscriminantAnalysis())\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Get LDA coefficients for feature importance\n",
    "    lda = pipeline.named_steps['lda']\n",
    "    scaler = pipeline.named_steps['scaler']\n",
    "    \n",
    "    # Get coefficient importance (use absolute values)\n",
    "    coef = np.abs(lda.coef_[0])\n",
    "    \n",
    "    # Scale coefficients by the scaler to get feature importance\n",
    "    scaled_coef = coef * scaler.scale_\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': scaled_coef\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'class_report': class_report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'cv_scores': cv_scores,\n",
    "        'feature_importance': feature_importance,\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "\n",
    "# Prepare data for models - filter features that exist in the dataset\n",
    "valid_perc_features = [f for f in selected_features_perc if f in df_filtered.columns]\n",
    "valid_net_features = [f for f in selected_features_net if f in df_filtered.columns]\n",
    "valid_deficit_features = [f for f in deficit_features if f in df_filtered.columns]\n",
    "\n",
    "print(f\"\\nValid features for perc_capacity: {len(valid_perc_features)}/{len(selected_features_perc)}\")\n",
    "print(f\"Valid features for net_rev_per_student: {len(valid_net_features)}/{len(selected_features_net)}\")\n",
    "print(f\"Valid features for deficit prediction: {len(valid_deficit_features)}/{len(deficit_features)}\")\n",
    "\n",
    "# Prepare data for perc_capacity model\n",
    "X_perc = df_filtered[valid_perc_features].copy()\n",
    "X_perc = X_perc.fillna(X_perc.mean())  # Fill missing values\n",
    "y_perc = df_filtered['perc_capacity_binary']\n",
    "\n",
    "# Prepare data for net_rev model\n",
    "X_net = df_filtered[valid_net_features].copy()\n",
    "X_net = X_net.fillna(X_net.mean())  # Fill missing values\n",
    "y_net = df_filtered['net_rev_binary']\n",
    "\n",
    "# Prepare data for deficit model\n",
    "X_deficit = df_filtered[valid_deficit_features].copy()\n",
    "X_deficit = X_deficit.fillna(X_deficit.mean())  # Fill missing values\n",
    "y_deficit = df_filtered['deficit_binary']\n",
    "\n",
    "# Build LDA models\n",
    "print(\"\\n==== Building LDA models ====\")\n",
    "lda_perc = build_lda_model(X_perc, y_perc, valid_perc_features)\n",
    "lda_net = build_lda_model(X_net, y_net, valid_net_features)\n",
    "lda_deficit = build_lda_model(X_deficit, y_deficit, valid_deficit_features)\n",
    "\n",
    "# Display results for perc_capacity\n",
    "print(\"\\n==== LDA Results for perc_capacity ====\")\n",
    "print(f\"Test Accuracy: {lda_perc['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {lda_perc['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {lda_perc['cv_scores'].mean():.4f} ± {lda_perc['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(lda_perc['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(lda_perc['feature_importance'].head(10))\n",
    "\n",
    "# Display results for net_rev\n",
    "print(\"\\n==== LDA Results for net_rev_per_student ====\")\n",
    "print(f\"Test Accuracy: {lda_net['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {lda_net['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {lda_net['cv_scores'].mean():.4f} ± {lda_net['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(lda_net['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(lda_net['feature_importance'].head(10))\n",
    "\n",
    "# Display results for deficit\n",
    "print(\"\\n==== LDA Results for deficit prediction ====\")\n",
    "print(f\"Test Accuracy: {lda_deficit['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {lda_deficit['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {lda_deficit['cv_scores'].mean():.4f} ± {lda_deficit['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(lda_deficit['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(lda_deficit['feature_importance'].head(10))\n",
    "\n",
    "# Visualize confusion matrices\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(lda_perc['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('LDA Confusion Matrix - perc_capacity')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.heatmap(lda_net['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('LDA Confusion Matrix - net_rev_per_student')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(lda_deficit['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('LDA Confusion Matrix - deficit')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lda_confusion_matrices.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize ROC curves\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(lda_perc['fpr'], lda_perc['tpr'], label=f'LDA (AUC = {lda_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - perc_capacity')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(lda_net['fpr'], lda_net['tpr'], label=f'LDA (AUC = {lda_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - net_rev_per_student')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(lda_deficit['fpr'], lda_deficit['tpr'], label=f'LDA (AUC = {lda_deficit[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - deficit')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lda_roc_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(18, 15))\n",
    "plt.subplot(3, 1, 1)\n",
    "top10_perc = lda_perc['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_perc)\n",
    "plt.title('Top 10 Features - perc_capacity LDA')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "top10_net = lda_net['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_net)\n",
    "plt.title('Top 10 Features - net_rev_per_student LDA')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "top10_deficit = lda_deficit['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_deficit)\n",
    "plt.title('Top 10 Features - deficit LDA')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lda_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# Compare model performance across targets\n",
    "models = ['perc_capacity', 'net_rev_per_student', 'deficit']\n",
    "accuracies = [\n",
    "    lda_perc['accuracy'], \n",
    "    lda_net['accuracy'],\n",
    "    lda_deficit['accuracy']\n",
    "]\n",
    "cv_accuracies = [\n",
    "    lda_perc['cv_scores'].mean(),\n",
    "    lda_net['cv_scores'].mean(),\n",
    "    lda_deficit['cv_scores'].mean()\n",
    "]\n",
    "roc_aucs = [\n",
    "    lda_perc['roc_auc'],\n",
    "    lda_net['roc_auc'],\n",
    "    lda_deficit['roc_auc']\n",
    "]\n",
    "\n",
    "# Create comprehensive performance dataframe\n",
    "lda_performance = pd.DataFrame({\n",
    "    'Target': models,\n",
    "    'Test Accuracy': accuracies,\n",
    "    'CV Accuracy': cv_accuracies,\n",
    "    'ROC AUC': roc_aucs\n",
    "})\n",
    "\n",
    "print(\"\\nLDA Performance Comparison:\")\n",
    "print(lda_performance.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Create bar chart of performance metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.3\n",
    "\n",
    "plt.bar(x - width, accuracies, width, label='Test Accuracy')\n",
    "plt.bar(x, cv_accuracies, width, label='CV Accuracy')\n",
    "plt.bar(x + width, roc_aucs, width, label='ROC AUC')\n",
    "\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Score')\n",
    "plt.title('LDA Model Performance Comparison')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('lda_performance_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLDA classification analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db593d9-e229-4ce2-95fa-7e70cb0edffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification Script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prepare binary target variables\n",
    "# Calculate median of target variables to split into binary classes\n",
    "perc_capacity_median = df_filtered['perc_capacity'].median()\n",
    "net_rev_median = df_filtered['net_rev_per_student'].median()\n",
    "\n",
    "# Create binary target variables\n",
    "df_filtered['perc_capacity_binary'] = (df_filtered['perc_capacity'] > perc_capacity_median).astype(int)\n",
    "df_filtered['net_rev_binary'] = (df_filtered['net_rev_per_student'] > net_rev_median).astype(int)\n",
    "# Create deficit binary target variable\n",
    "df_filtered['deficit_binary'] = (df_filtered['net_program_80'] < 0).astype(int)\n",
    "\n",
    "print(\"Class distribution for binary target variables:\")\n",
    "print(\"\\nperc_capacity_binary:\")\n",
    "print(df_filtered['perc_capacity_binary'].value_counts())\n",
    "\n",
    "print(\"\\nnet_rev_binary:\")\n",
    "print(df_filtered['net_rev_binary'].value_counts())\n",
    "\n",
    "print(\"\\ndeficit_binary:\")\n",
    "print(df_filtered['deficit_binary'].value_counts())\n",
    "print(f\"Percentage of schools with deficit: {df_filtered['deficit_binary'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Define features for deficit prediction - exclude net_program_80 which is used to calculate the target\n",
    "deficit_features = [f for f in selected_features_net if f != 'net_program_80']\n",
    "\n",
    "# Function to build and evaluate Random Forest model with ROC AUC calculation\n",
    "def build_rf_model(X, y, feature_names):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Create a pipeline with scaling and Random Forest\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Get Random Forest feature importance\n",
    "    rf = pipeline.named_steps['rf']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'class_report': class_report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'cv_scores': cv_scores,\n",
    "        'feature_importance': feature_importance,\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "\n",
    "# Prepare data for models - filter features that exist in the dataset\n",
    "valid_perc_features = [f for f in selected_features_perc if f in df_filtered.columns]\n",
    "valid_net_features = [f for f in selected_features_net if f in df_filtered.columns]\n",
    "valid_deficit_features = [f for f in deficit_features if f in df_filtered.columns]\n",
    "\n",
    "print(f\"\\nValid features for perc_capacity: {len(valid_perc_features)}/{len(selected_features_perc)}\")\n",
    "print(f\"Valid features for net_rev_per_student: {len(valid_net_features)}/{len(selected_features_net)}\")\n",
    "print(f\"Valid features for deficit prediction: {len(valid_deficit_features)}/{len(deficit_features)}\")\n",
    "\n",
    "# Prepare data for perc_capacity model\n",
    "X_perc = df_filtered[valid_perc_features].copy()\n",
    "X_perc = X_perc.fillna(X_perc.mean())  # Fill missing values\n",
    "y_perc = df_filtered['perc_capacity_binary']\n",
    "\n",
    "# Prepare data for net_rev model\n",
    "X_net = df_filtered[valid_net_features].copy()\n",
    "X_net = X_net.fillna(X_net.mean())  # Fill missing values\n",
    "y_net = df_filtered['net_rev_binary']\n",
    "\n",
    "# Prepare data for deficit model\n",
    "X_deficit = df_filtered[valid_deficit_features].copy()\n",
    "X_deficit = X_deficit.fillna(X_deficit.mean())  # Fill missing values\n",
    "y_deficit = df_filtered['deficit_binary']\n",
    "\n",
    "# Build Random Forest models\n",
    "print(\"\\n==== Building Random Forest models ====\")\n",
    "rf_perc = build_rf_model(X_perc, y_perc, valid_perc_features)\n",
    "rf_net = build_rf_model(X_net, y_net, valid_net_features)\n",
    "rf_deficit = build_rf_model(X_deficit, y_deficit, valid_deficit_features)\n",
    "\n",
    "# Display results for perc_capacity\n",
    "print(\"\\n==== Random Forest Results for perc_capacity ====\")\n",
    "print(f\"Test Accuracy: {rf_perc['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {rf_perc['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {rf_perc['cv_scores'].mean():.4f} ± {rf_perc['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(rf_perc['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(rf_perc['feature_importance'].head(10))\n",
    "\n",
    "# Display results for net_rev\n",
    "print(\"\\n==== Random Forest Results for net_rev_per_student ====\")\n",
    "print(f\"Test Accuracy: {rf_net['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {rf_net['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {rf_net['cv_scores'].mean():.4f} ± {rf_net['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(rf_net['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(rf_net['feature_importance'].head(10))\n",
    "\n",
    "# Display results for deficit\n",
    "print(\"\\n==== Random Forest Results for deficit prediction ====\")\n",
    "print(f\"Test Accuracy: {rf_deficit['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {rf_deficit['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {rf_deficit['cv_scores'].mean():.4f} ± {rf_deficit['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(rf_deficit['class_report'])\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(rf_deficit['feature_importance'].head(10))\n",
    "\n",
    "# Visualize confusion matrices\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(rf_perc['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('RF Confusion Matrix - perc_capacity')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.heatmap(rf_net['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('RF Confusion Matrix - net_rev_per_student')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(rf_deficit['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('RF Confusion Matrix - deficit')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_confusion_matrices.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize ROC curves\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rf_perc['fpr'], rf_perc['tpr'], label=f'RF (AUC = {rf_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - perc_capacity')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(rf_net['fpr'], rf_net['tpr'], label=f'RF (AUC = {rf_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - net_rev_per_student')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(rf_deficit['fpr'], rf_deficit['tpr'], label=f'RF (AUC = {rf_deficit[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - deficit')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_roc_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(18, 15))\n",
    "plt.subplot(3, 1, 1)\n",
    "top10_perc = rf_perc['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_perc)\n",
    "plt.title('Top 10 Features - perc_capacity RF')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "top10_net = rf_net['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_net)\n",
    "plt.title('Top 10 Features - net_rev_per_student RF')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "top10_deficit = rf_deficit['feature_importance'].head(10)\n",
    "sns.barplot(x='Importance', y='Feature', data=top10_deficit)\n",
    "plt.title('Top 10 Features - deficit RF')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# Compare model performance across targets\n",
    "models = ['perc_capacity', 'net_rev_per_student', 'deficit']\n",
    "accuracies = [\n",
    "    rf_perc['accuracy'], \n",
    "    rf_net['accuracy'],\n",
    "    rf_deficit['accuracy']\n",
    "]\n",
    "cv_accuracies = [\n",
    "    rf_perc['cv_scores'].mean(),\n",
    "    rf_net['cv_scores'].mean(),\n",
    "    rf_deficit['cv_scores'].mean()\n",
    "]\n",
    "roc_aucs = [\n",
    "    rf_perc['roc_auc'],\n",
    "    rf_net['roc_auc'],\n",
    "    rf_deficit['roc_auc']\n",
    "]\n",
    "\n",
    "# Create comprehensive performance dataframe\n",
    "rf_performance = pd.DataFrame({\n",
    "    'Target': models,\n",
    "    'Test Accuracy': accuracies,\n",
    "    'CV Accuracy': cv_accuracies,\n",
    "    'ROC AUC': roc_aucs\n",
    "})\n",
    "\n",
    "print(\"\\nRandom Forest Performance Comparison:\")\n",
    "print(rf_performance.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Create bar chart of performance metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.3\n",
    "\n",
    "plt.bar(x - width, accuracies, width, label='Test Accuracy')\n",
    "plt.bar(x, cv_accuracies, width, label='CV Accuracy')\n",
    "plt.bar(x + width, roc_aucs, width, label='ROC AUC')\n",
    "\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Random Forest Model Performance Comparison')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('rf_performance_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRandom Forest classification analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492e86d-54af-4522-addc-5e940ae8cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Classification Script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prepare binary target variables\n",
    "# Calculate median of target variables to split into binary classes\n",
    "perc_capacity_median = df_filtered['perc_capacity'].median()\n",
    "net_rev_median = df_filtered['net_rev_per_student'].median()\n",
    "\n",
    "# Create binary target variables\n",
    "df_filtered['perc_capacity_binary'] = (df_filtered['perc_capacity'] > perc_capacity_median).astype(int)\n",
    "df_filtered['net_rev_binary'] = (df_filtered['net_rev_per_student'] > net_rev_median).astype(int)\n",
    "# Create deficit binary target variable\n",
    "df_filtered['deficit_binary'] = (df_filtered['net_program_80'] < 0).astype(int)\n",
    "\n",
    "print(\"Class distribution for binary target variables:\")\n",
    "print(\"\\nperc_capacity_binary:\")\n",
    "print(df_filtered['perc_capacity_binary'].value_counts())\n",
    "\n",
    "print(\"\\nnet_rev_binary:\")\n",
    "print(df_filtered['net_rev_binary'].value_counts())\n",
    "\n",
    "print(\"\\ndeficit_binary:\")\n",
    "print(df_filtered['deficit_binary'].value_counts())\n",
    "print(f\"Percentage of schools with deficit: {df_filtered['deficit_binary'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Define features for deficit prediction - exclude net_program_80 which is used to calculate the target\n",
    "deficit_features = [f for f in selected_features_net if f != 'net_program_80']\n",
    "\n",
    "# Function to build and evaluate SVM model with ROC AUC calculation\n",
    "def build_svm_model(X, y, feature_names):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    # Create a pipeline with scaling and SVM\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(probability=True, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # SVM doesn't provide direct feature importance\n",
    "    # We could use permutation importance but will leave it for simplicity\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'class_report': class_report,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'cv_scores': cv_scores,\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "\n",
    "# Prepare data for models - filter features that exist in the dataset\n",
    "valid_perc_features = [f for f in selected_features_perc if f in df_filtered.columns]\n",
    "valid_net_features = [f for f in selected_features_net if f in df_filtered.columns]\n",
    "valid_deficit_features = [f for f in deficit_features if f in df_filtered.columns]\n",
    "\n",
    "print(f\"\\nValid features for perc_capacity: {len(valid_perc_features)}/{len(selected_features_perc)}\")\n",
    "print(f\"Valid features for net_rev_per_student: {len(valid_net_features)}/{len(selected_features_net)}\")\n",
    "print(f\"Valid features for deficit prediction: {len(valid_deficit_features)}/{len(deficit_features)}\")\n",
    "\n",
    "# Prepare data for perc_capacity model\n",
    "X_perc = df_filtered[valid_perc_features].copy()\n",
    "X_perc = X_perc.fillna(X_perc.mean())  # Fill missing values\n",
    "y_perc = df_filtered['perc_capacity_binary']\n",
    "\n",
    "# Prepare data for net_rev model\n",
    "X_net = df_filtered[valid_net_features].copy()\n",
    "X_net = X_net.fillna(X_net.mean())  # Fill missing values\n",
    "y_net = df_filtered['net_rev_binary']\n",
    "\n",
    "# Prepare data for deficit model\n",
    "X_deficit = df_filtered[valid_deficit_features].copy()\n",
    "X_deficit = X_deficit.fillna(X_deficit.mean())  # Fill missing values\n",
    "y_deficit = df_filtered['deficit_binary']\n",
    "\n",
    "# Build SVM models\n",
    "print(\"\\n==== Building SVM models ====\")\n",
    "svm_perc = build_svm_model(X_perc, y_perc, valid_perc_features)\n",
    "svm_net = build_svm_model(X_net, y_net, valid_net_features)\n",
    "svm_deficit = build_svm_model(X_deficit, y_deficit, valid_deficit_features)\n",
    "\n",
    "# Display results for perc_capacity\n",
    "print(\"\\n==== SVM Results for perc_capacity ====\")\n",
    "print(f\"Test Accuracy: {svm_perc['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {svm_perc['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {svm_perc['cv_scores'].mean():.4f} ± {svm_perc['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(svm_perc['class_report'])\n",
    "\n",
    "# Display results for net_rev\n",
    "print(\"\\n==== SVM Results for net_rev_per_student ====\")\n",
    "print(f\"Test Accuracy: {svm_net['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {svm_net['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {svm_net['cv_scores'].mean():.4f} ± {svm_net['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(svm_net['class_report'])\n",
    "\n",
    "# Display results for deficit\n",
    "print(\"\\n==== SVM Results for deficit prediction ====\")\n",
    "print(f\"Test Accuracy: {svm_deficit['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {svm_deficit['roc_auc']:.4f}\")\n",
    "print(f\"Cross-Validation Accuracy: {svm_deficit['cv_scores'].mean():.4f} ± {svm_deficit['cv_scores'].std():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(svm_deficit['class_report'])\n",
    "\n",
    "# Visualize confusion matrices\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.heatmap(svm_perc['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('SVM Confusion Matrix - perc_capacity')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.heatmap(svm_net['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('SVM Confusion Matrix - net_rev_per_student')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(svm_deficit['conf_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('SVM Confusion Matrix - deficit')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('svm_confusion_matrices.png')\n",
    "plt.show()\n",
    "\n",
    "# Visualize ROC curves\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(svm_perc['fpr'], svm_perc['tpr'], label=f'SVM (AUC = {svm_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - perc_capacity')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(svm_net['fpr'], svm_net['tpr'], label=f'SVM (AUC = {svm_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - net_rev_per_student')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(svm_deficit['fpr'], svm_deficit['tpr'], label=f'SVM (AUC = {svm_deficit[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - deficit')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('svm_roc_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Compare model performance across targets\n",
    "models = ['perc_capacity', 'net_rev_per_student', 'deficit']\n",
    "accuracies = [\n",
    "    svm_perc['accuracy'], \n",
    "    svm_net['accuracy'],\n",
    "    svm_deficit['accuracy']\n",
    "]\n",
    "cv_accuracies = [\n",
    "    svm_perc['cv_scores'].mean(),\n",
    "    svm_net['cv_scores'].mean(),\n",
    "    svm_deficit['cv_scores'].mean()\n",
    "]\n",
    "roc_aucs = [\n",
    "    svm_perc['roc_auc'],\n",
    "    svm_net['roc_auc'],\n",
    "    svm_deficit['roc_auc']\n",
    "]\n",
    "\n",
    "# Create comprehensive performance dataframe\n",
    "svm_performance = pd.DataFrame({\n",
    "    'Target': models,\n",
    "    'Test Accuracy': accuracies,\n",
    "    'CV Accuracy': cv_accuracies,\n",
    "    'ROC AUC': roc_aucs\n",
    "})\n",
    "\n",
    "print(\"\\nSVM Performance Comparison:\")\n",
    "print(svm_performance.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Create bar chart of performance metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.3\n",
    "\n",
    "plt.bar(x - width, accuracies, width, label='Test Accuracy')\n",
    "plt.bar(x, cv_accuracies, width, label='CV Accuracy')\n",
    "plt.bar(x + width, roc_aucs, width, label='ROC AUC')\n",
    "\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Score')\n",
    "plt.title('SVM Model Performance Comparison')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('svm_performance_comparison.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716877b3-ca58-4a56-9edb-54ff94635bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Model Comparison Script\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This script assumes you have already run the LDA, RF, and SVM scripts\n",
    "# and have the results stored in variables like lda_perc, rf_perc, svm_perc, etc.\n",
    "\n",
    "# Create function to extract metrics from classification reports\n",
    "def extract_metrics_from_report(report):\n",
    "    lines = report.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if ' 1 ' in line:  # Class 1 metrics\n",
    "            parts = [p for p in line.split() if p]\n",
    "            precision = float(parts[1])\n",
    "            recall = float(parts[2])\n",
    "            f1 = float(parts[3])\n",
    "            return precision, recall, f1\n",
    "    return None, None, None\n",
    "\n",
    "# Function to create comparison dataframe for a specific target\n",
    "def create_comparison_df(target_name, lda_results, rf_results, svm_results):\n",
    "    # Extract precision, recall, and F1 for each model\n",
    "    lda_precision, lda_recall, lda_f1 = extract_metrics_from_report(lda_results['class_report'])\n",
    "    rf_precision, rf_recall, rf_f1 = extract_metrics_from_report(rf_results['class_report'])\n",
    "    svm_precision, svm_recall, svm_f1 = extract_metrics_from_report(svm_results['class_report'])\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Algorithm': ['LDA', 'Random Forest', 'SVM'],\n",
    "        'Test Accuracy': [lda_results['accuracy'], rf_results['accuracy'], svm_results['accuracy']],\n",
    "        'CV Accuracy': [lda_results['cv_scores'].mean(), rf_results['cv_scores'].mean(), svm_results['cv_scores'].mean()],\n",
    "        'ROC AUC': [lda_results['roc_auc'], rf_results['roc_auc'], svm_results['roc_auc']],\n",
    "        'Precision': [lda_precision, rf_precision, svm_precision],\n",
    "        'Recall': [lda_recall, rf_recall, svm_recall],\n",
    "        'F1-Score': [lda_f1, rf_f1, svm_f1]\n",
    "    })\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Create comparison dataframes for each target\n",
    "perc_capacity_comparison = create_comparison_df(\n",
    "    'perc_capacity', \n",
    "    lda_perc, \n",
    "    rf_perc, \n",
    "    svm_perc\n",
    ")\n",
    "\n",
    "net_rev_comparison = create_comparison_df(\n",
    "    'net_rev_per_student', \n",
    "    lda_net, \n",
    "    rf_net, \n",
    "    svm_net\n",
    ")\n",
    "\n",
    "deficit_comparison = create_comparison_df(\n",
    "    'deficit', \n",
    "    lda_deficit, \n",
    "    rf_deficit, \n",
    "    svm_deficit\n",
    ")\n",
    "\n",
    "# Display comparison results\n",
    "print(\"\\n==== Model Comparison for Percent Capacity ====\")\n",
    "print(perc_capacity_comparison.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "print(\"\\n==== Model Comparison for Net Revenue ====\")\n",
    "print(net_rev_comparison.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "print(\"\\n==== Model Comparison for Deficit ====\")\n",
    "print(deficit_comparison.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "# Visualize model comparisons\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='Algorithm', y='Test Accuracy', data=perc_capacity_comparison)\n",
    "plt.title('Test Accuracy - Percent Capacity')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x='Algorithm', y='Test Accuracy', data=net_rev_comparison)\n",
    "plt.title('Test Accuracy - Net Revenue')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='Algorithm', y='Test Accuracy', data=deficit_comparison)\n",
    "plt.title('Test Accuracy - Deficit')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_accuracy_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# ROC AUC comparison\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='Algorithm', y='ROC AUC', data=perc_capacity_comparison)\n",
    "plt.title('ROC AUC - Percent Capacity')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x='Algorithm', y='ROC AUC', data=net_rev_comparison)\n",
    "plt.title('ROC AUC - Net Revenue')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='Algorithm', y='ROC AUC', data=deficit_comparison)\n",
    "plt.title('ROC AUC - Deficit')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_auc_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# F1 Score comparison\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x='Algorithm', y='F1-Score', data=perc_capacity_comparison)\n",
    "plt.title('F1 Score - Percent Capacity')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.barplot(x='Algorithm', y='F1-Score', data=net_rev_comparison)\n",
    "plt.title('F1 Score - Net Revenue')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.barplot(x='Algorithm', y='F1-Score', data=deficit_comparison)\n",
    "plt.title('F1 Score - Deficit')\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_score_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Create radar charts for comprehensive model comparison\n",
    "def create_radar_chart(target_comparison, title):\n",
    "    # Extract metrics for radar chart\n",
    "    labels = ['Accuracy', 'ROC AUC', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    # Get values for each algorithm\n",
    "    lda_values = [\n",
    "        target_comparison.loc[0, 'Test Accuracy'],\n",
    "        target_comparison.loc[0, 'ROC AUC'],\n",
    "        target_comparison.loc[0, 'Precision'],\n",
    "        target_comparison.loc[0, 'Recall'],\n",
    "        target_comparison.loc[0, 'F1-Score']\n",
    "    ]\n",
    "    \n",
    "    rf_values = [\n",
    "        target_comparison.loc[1, 'Test Accuracy'],\n",
    "        target_comparison.loc[1, 'ROC AUC'],\n",
    "        target_comparison.loc[1, 'Precision'],\n",
    "        target_comparison.loc[1, 'Recall'],\n",
    "        target_comparison.loc[1, 'F1-Score']\n",
    "    ]\n",
    "    \n",
    "    svm_values = [\n",
    "        target_comparison.loc[2, 'Test Accuracy'],\n",
    "        target_comparison.loc[2, 'ROC AUC'],\n",
    "        target_comparison.loc[2, 'Precision'],\n",
    "        target_comparison.loc[2, 'Recall'],\n",
    "        target_comparison.loc[2, 'F1-Score']\n",
    "    ]\n",
    "    \n",
    "    # Set up the radar chart\n",
    "    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Add the last value to close the loop for each algorithm\n",
    "    lda_values += lda_values[:1]\n",
    "    rf_values += rf_values[:1]\n",
    "    svm_values += svm_values[:1]\n",
    "    labels += labels[:1]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Plot each algorithm\n",
    "    ax.plot(angles, lda_values, 'o-', linewidth=2, label='LDA')\n",
    "    ax.plot(angles, rf_values, 'o-', linewidth=2, label='Random Forest')\n",
    "    ax.plot(angles, svm_values, 'o-', linewidth=2, label='SVM')\n",
    "    \n",
    "    # Fill area\n",
    "    ax.fill(angles, lda_values, alpha=0.1)\n",
    "    ax.fill(angles, rf_values, alpha=0.1)\n",
    "    ax.fill(angles, svm_values, alpha=0.1)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels[:-1])\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0.5, 1.0)\n",
    "    \n",
    "    # Add legend and title\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create radar charts for each target\n",
    "perc_radar = create_radar_chart(perc_capacity_comparison, 'Model Comparison - Percent Capacity')\n",
    "perc_radar.savefig('perc_capacity_radar.png')\n",
    "plt.close(perc_radar)\n",
    "\n",
    "net_rev_radar = create_radar_chart(net_rev_comparison, 'Model Comparison - Net Revenue')\n",
    "net_rev_radar.savefig('net_rev_radar.png')\n",
    "plt.close(net_rev_radar)\n",
    "\n",
    "deficit_radar = create_radar_chart(deficit_comparison, 'Model Comparison - Deficit')\n",
    "deficit_radar.savefig('deficit_radar.png')\n",
    "plt.close(deficit_radar)\n",
    "\n",
    "# Create a final combined ROC curve plot for all models and targets\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Percent Capacity ROC curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(lda_perc['fpr'], lda_perc['tpr'], label=f'LDA (AUC = {lda_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot(rf_perc['fpr'], rf_perc['tpr'], label=f'RF (AUC = {rf_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot(svm_perc['fpr'], svm_perc['tpr'], label=f'SVM (AUC = {svm_perc[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Percent Capacity')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Net Revenue ROC curves\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(lda_net['fpr'], lda_net['tpr'], label=f'LDA (AUC = {lda_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot(rf_net['fpr'], rf_net['tpr'], label=f'RF (AUC = {rf_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot(svm_net['fpr'], svm_net['tpr'], label=f'SVM (AUC = {svm_net[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Net Revenue')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Deficit ROC curves\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(lda_deficit['fpr'], lda_deficit['tpr'], label=f'LDA (AUC = {lda_deficit[\"roc_auc\"]:.3f})')\n",
    "plt.plot(rf_deficit['fpr'], rf_deficit['tpr'], label=f'RF (AUC = {rf_deficit[\"roc_auc\"]:.3f})')\n",
    "plt.plot(svm_deficit['fpr'], svm_deficit['tpr'], label=f'SVM (AUC = {svm_deficit[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Deficit')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('combined_roc_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# Identify best algorithm for each target based on ROC AUC\n",
    "best_perc_algo = perc_capacity_comparison.loc[perc_capacity_comparison['ROC AUC'].idxmax(), 'Algorithm']\n",
    "best_perc_auc = perc_capacity_comparison['ROC AUC'].max()\n",
    "best_net_algo = net_rev_comparison.loc[net_rev_comparison['ROC AUC'].idxmax(), 'Algorithm']\n",
    "best_net_auc = net_rev_comparison['ROC AUC'].max()\n",
    "best_deficit_algo = deficit_comparison.loc[deficit_comparison['ROC AUC'].idxmax(), 'Algorithm']\n",
    "best_deficit_auc = deficit_comparison['ROC AUC'].max()\n",
    "\n",
    "print(\"\\n==== Best Models Based on ROC AUC ====\")\n",
    "print(f\"Percent Capacity: {best_perc_algo} (AUC = {best_perc_auc:.4f})\")\n",
    "print(f\"Net Revenue: {best_net_algo} (AUC = {best_net_auc:.4f})\")\n",
    "print(f\"Deficit: {best_deficit_algo} (AUC = {best_deficit_auc:.4f})\")\n",
    "\n",
    "# Create a final summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Target': ['Percent Capacity', 'Net Revenue', 'Deficit'],\n",
    "    'Best Algorithm': [best_perc_algo, best_net_algo, best_deficit_algo],\n",
    "    'ROC AUC': [best_perc_auc, best_net_auc, best_deficit_auc]\n",
    "})\n",
    "\n",
    "print(\"\\n==== Final Model Selection Summary ====\")\n",
    "print(summary_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "\n",
    "print(\"\\nModel comparison analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99710903-2827-4ef6-8ec4-507e34f5b430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619c9c3-c35f-4e6e-b9b9-ecef9f02618e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
